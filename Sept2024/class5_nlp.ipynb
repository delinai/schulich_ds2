{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2024.9.11-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/delinaivanova/Library/Python/3.9/lib/python/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: joblib in /Users/delinaivanova/Library/Python/3.9/lib/python/site-packages (from nltk) (1.3.2)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 888 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.9.1 regex-2024.9.11\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/delinaivanova/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/delinaivanova/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/delinaivanova/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming:\n",
    "Definition: Stemming is a crude heuristic process that chops off the ends of words to reduce them to their base form, often resulting in words that are not valid or recognized words. It doesn’t consider the context in which a word is used.\n",
    "Example:\n",
    "\"Running\" → \"Run\"\n",
    "\"Studies\" → \"Studi\"\n",
    "\"Happily\" → \"Happili\"\n",
    "Pros: Stemming is typically faster since it's just cutting off word suffixes using predefined rules.\n",
    "Cons: The output may not always be linguistically correct (e.g., \"happily\" becomes \"happili\" or \"studies\" becomes \"studi\").\n",
    "\n",
    "#### Lemmatization:\n",
    "Definition: Lemmatization is a more sophisticated process that reduces words to their dictionary form (lemma), considering the context and part of speech. It ensures that the base form is an actual word.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"Running\" → \"Run\"\n",
    "\"Studies\" → \"Study\"\n",
    "\"Better\" → \"Good\"\n",
    "Pros: Lemmatization produces more accurate base forms of words, which are valid and meaningful.\n",
    "\n",
    "Cons: It is slower compared to stemming, as it needs to analyze the word and sometimes the sentence to determine the part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'ate', 'an', 'apple', 'today', '!', 'It', 'was', 'red', 'and', 'delicious', '.']\n",
      "Tokens after removing punctuation: ['i', 'ate', 'an', 'apple', 'today', 'it', 'was', 'red', 'and', 'delicious']\n",
      "Filtered Tokens (no stopwords): ['ate', 'apple', 'today', 'red', 'delicious']\n",
      "Stemmed Tokens: ['ate', 'appl', 'today', 'red', 'delici']\n",
      "Lemmatized Tokens: ['ate', 'apple', 'today', 'red', 'delicious']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"I ate an apple today! It was red and delicious.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert to lowercase and remove punctuation\n",
    "tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "print(\"Tokens after removing punctuation:\", tokens)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"Filtered Tokens (no stopwords):\", filtered_tokens)\n",
    "\n",
    "# Apply stemming and lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words: simply counts the number of times words appear.\n",
    "N-gram: looks at compbinations of words and how often they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Model:\n",
      "    amazing  and  as  did  disappointing  easy  everyone  expected  fantastic  \\\n",
      "0        1    1   0    0              0     1         0         0          0   \n",
      "1        0    0   1    1              1     0         0         1          0   \n",
      "2        0    0   0    0              0     0         1         0          1   \n",
      "3        0    0   0    0              0     0         0         0          0   \n",
      "\n",
      "   is  ...  product  recommend  terrible  the  to  use  very  waste  work  \\\n",
      "0   1  ...        1          0         0    1   1    1     0      0     0   \n",
      "1   0  ...        1          0         0    1   0    0     1      0     1   \n",
      "2   0  ...        1          1         0    0   1    0     0      0     0   \n",
      "3   0  ...        1          0         1    0   0    0     0      1     0   \n",
      "\n",
      "   would  \n",
      "0      0  \n",
      "1      0  \n",
      "2      1  \n",
      "3      0  \n",
      "\n",
      "[4 rows x 25 columns]\n",
      "N-Gram Model (Bigrams):\n",
      "    amazing and  and easy  as expected  did not  easy to  expected very  \\\n",
      "0            1         1            0        0        1              0   \n",
      "1            0         0            1        1        0              1   \n",
      "2            0         0            0        0        0              0   \n",
      "3            0         0            0        0        0              0   \n",
      "\n",
      "   fantastic product  is amazing  it is  it to  ...  product would  \\\n",
      "0                  0           1      1      0  ...              0   \n",
      "1                  0           0      0      0  ...              0   \n",
      "2                  1           0      0      1  ...              1   \n",
      "3                  0           0      0      0  ...              0   \n",
      "\n",
      "   recommend it  terrible product  the product  to everyone  to use  \\\n",
      "0             0                 0            1            0       1   \n",
      "1             0                 0            1            0       0   \n",
      "2             1                 0            0            1       0   \n",
      "3             0                 1            0            0       0   \n",
      "\n",
      "   very disappointing  waste of  work as  would recommend  \n",
      "0                   0         0        0                0  \n",
      "1                   1         0        1                0  \n",
      "2                   0         0        0                1  \n",
      "3                   0         1        0                0  \n",
      "\n",
      "[4 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample data\n",
    "documents = [\n",
    "    \"I love the product. It is amazing and easy to use.\",\n",
    "    \"The product did not work as expected. Very disappointing.\",\n",
    "    \"Fantastic product! I would recommend it to everyone.\",\n",
    "    \"Terrible product. Waste of money.\"\n",
    "]\n",
    "\n",
    "# Create Bag-of-Words model\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame for better understanding\n",
    "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
    "print(\"Bag-of-Words Model:\\n\", bow_df)\n",
    "\n",
    "# Create an N-Gram model (bigram)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(2, 2))  # Using bigrams\n",
    "X_ngram = vectorizer_ngram.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame for better understanding\n",
    "ngram_df = pd.DataFrame(X_ngram.toarray(), columns=vectorizer_ngram.get_feature_names_out())\n",
    "print(\"N-Gram Model (Bigrams):\\n\", ngram_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the TF-IDF Output Shows:\n",
    "The TF-IDF matrix is a 4x25 table where:\n",
    "\n",
    "* Rows: Each row represents a document from the input documents. In this case, there are 4 rows, meaning you have 4 documents.\n",
    "* Columns: Each column represents a unique word from the corpus (all the documents combined). In this case, there are 25 unique words across all documents.\n",
    "* Values (TF-IDF Scores): The values in the matrix are the TF-IDF scores for each word in each document. The TF-IDF score tells us how important a word is in a particular document relative to the entire corpus.\n",
    "\n",
    "Key Points to Consider:\n",
    "* High TF-IDF Score: A higher score indicates that the word is important to that specific document but not very common across all documents.\n",
    "* Low TF-IDF Score: A lower score indicates that the word is either common across all documents or is not particularly important in this document.\n",
    "* Zeros: A score of 0 means the word does not appear in that document.\n",
    "\n",
    "* High TF-IDF Score: Indicates that the word is frequent in a specific document (high TF) but rare across the entire corpus (high IDF). Such words are considered important because they capture the document's unique content.\n",
    "* Low TF-IDF Score: Indicates either that the word is common across many documents (low IDF) or that it appears rarely within the document (low TF). Such words are not considered important or distinctive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model:\n",
      "     amazing       and        as       did  disappointing      easy  everyone  \\\n",
      "0  0.350562  0.350562  0.000000  0.000000       0.000000  0.350562  0.000000   \n",
      "1  0.000000  0.000000  0.355921  0.355921       0.355921  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.000000       0.000000  0.000000  0.425802   \n",
      "3  0.000000  0.000000  0.000000  0.000000       0.000000  0.000000  0.000000   \n",
      "\n",
      "   expected  fantastic        is  ...   product  recommend  terrible  \\\n",
      "0  0.000000   0.000000  0.350562  ...  0.182938   0.000000  0.000000   \n",
      "1  0.355921   0.000000  0.000000  ...  0.185734   0.000000  0.000000   \n",
      "2  0.000000   0.425802  0.000000  ...  0.222201   0.425802  0.000000   \n",
      "3  0.000000   0.000000  0.000000  ...  0.252468   0.000000  0.483803   \n",
      "\n",
      "        the        to       use      very     waste      work     would  \n",
      "0  0.276387  0.276387  0.350562  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.280612  0.000000  0.000000  0.355921  0.000000  0.355921  0.000000  \n",
      "2  0.000000  0.335707  0.000000  0.000000  0.000000  0.000000  0.425802  \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.483803  0.000000  0.000000  \n",
      "\n",
      "[4 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF model\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame for better understanding\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF Model:\\n\", tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 946 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/delinaivanova/Library/Python/3.9/lib/python/site-packages (from gensim) (1.13.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 573 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.18.5 in /Users/delinaivanova/Library/Python/3.9/lib/python/site-packages (from gensim) (1.26.4)\n",
      "Collecting wrapt\n",
      "  Downloading wrapt-1.16.0-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, smart-open, gensim\n",
      "Successfully installed gensim-4.3.3 smart-open-7.0.5 wrapt-1.16.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding for 'product':\n",
      " [-0.01632074  0.0089843  -0.00828079  0.00164966  0.01698902 -0.00892575\n",
      "  0.00904536 -0.01356366 -0.00710585  0.01880129 -0.00315039  0.0006427\n",
      " -0.00827438 -0.01536828 -0.00301453  0.00495137 -0.00178086  0.01107326\n",
      " -0.00549231  0.00450072  0.01091657  0.0166995  -0.00290105 -0.0184196\n",
      "  0.00874619  0.00114292  0.01488733 -0.00162391 -0.00528543 -0.01750779\n",
      " -0.00171116  0.00566212  0.01080635  0.0141175  -0.01139941  0.00372219\n",
      "  0.01218469 -0.00959842 -0.0062074   0.01359864  0.00326303  0.00038398\n",
      "  0.00693083  0.00044581  0.01924413  0.01012162 -0.0178444  -0.01409721\n",
      "  0.00179413  0.01279023]\n",
      "Words similar to 'product': [('work', 0.230143204331398), ('Waste', 0.2206723541021347), ('it', 0.21899765729904175), ('Very', 0.1608201116323471), ('expected', 0.14899705350399017)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample data (tokenized)\n",
    "sentences = [\n",
    "    word_tokenize(\"I love the product. It is amazing and easy to use.\"),\n",
    "    word_tokenize(\"The product did not work as expected. Very disappointing.\"),\n",
    "    word_tokenize(\"Fantastic product! I would recommend it to everyone.\"),\n",
    "    word_tokenize(\"Terrible product. Waste of money.\")\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get word embeddings for \"product\"\n",
    "print(\"Word Embedding for 'product':\\n\", word2vec_model.wv['product'])\n",
    "\n",
    "# Find similar words to \"product\"\n",
    "similar_words = word2vec_model.wv.most_similar('product', topn=5)\n",
    "print(\"Words similar to 'product':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate more synthetic data\n",
    "data = {\n",
    "    'Review': [\n",
    "        'I love the product! It works great and is very easy to use.',\n",
    "        'The product was okay, but I had some issues with customer service.',\n",
    "        'I am very disappointed. The product stopped working after two weeks.',\n",
    "        'Fantastic product! Will definitely buy again.',\n",
    "        'Not worth the money. Very poor quality.',\n",
    "        'Absolutely horrible experience. The product broke after one day.',\n",
    "        'I would recommend this product to everyone. Best purchase I ever made!',\n",
    "        'Customer service was great, but the product was just average.',\n",
    "        'The product exceeded my expectations! Worth every penny.',\n",
    "        'Terrible! The product didn’t work as advertised.'\n",
    "    ],\n",
    "    'Sentiment': [1, 0, 0, 1, 0, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame\n",
    "reviews_df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Classifier Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Prepare the data\n",
    "documents = reviews_df['Review'].tolist()\n",
    "labels = reviews_df['Sentiment'].tolist()\n",
    "\n",
    "# Tokenize sentences for Word2Vec model\n",
    "sentences = [word_tokenize(doc) for doc in documents]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to convert a document to its embedding representation (summing vectors)\n",
    "def document_embedding(doc):\n",
    "    tokens = word_tokenize(doc)\n",
    "    valid_words = [word for word in tokens if word in word2vec_model.wv]\n",
    "    if valid_words:\n",
    "        return np.sum([word2vec_model.wv[word] for word in valid_words], axis=0)  # Sum instead of mean\n",
    "    else:\n",
    "        return np.zeros((50,))\n",
    "\n",
    "# Create feature matrix\n",
    "X = np.array([document_embedding(doc) for doc in documents])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple Logistic Regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Sentiment Classifier Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
